{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SE_25 Mitchell Thomson SoundHunt\n",
    "## A Sound Organization Project\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soundhunt attempts to cluster like sounds by reading the chromavector of sound files, clustering, then reducing the dimension of the 12 note matrix. The main question is there a way to use these methods above to get accurate comparison measurement? The methods above proved effective for comparing very distinct sounds but proved to vague of information to compare similiar yet different sound files entirely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the 'SoundHunt' project is to build a tool that will allow sound designers faster access to their sounds and giving them a better organization method based on actual sound data rather than a user tagging the category. What is an efficent way to import, read, analyse and categorize large amounts of sound files in a way that would be both effective and useable for sound designers? Breaking down this question poses alot of challenges and many different ways to approach it, however that is what the project goal was. The main goal and question for this jupyter notebook is to determine the a way to first find a comparison between sound data and then how can this compared data be clustered/organized in a meaningful way? The methods that will be showcased are FFT, feature extraction,dimension reduction, chromavector comparison, and Kmeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "from scipy.fftpack import fft,fftfreq\n",
    "import numpy as np\n",
    "import pandas as pb\n",
    "import wavio\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.cluster import homogeneity_score\n",
    "from pyAudioAnalysis import audioBasicIO\n",
    "from pyAudioAnalysis import ShortTermFeatures\n",
    "from pyAudioAnalysis import Myfeature\n",
    "import librosa\n",
    "import librosa.display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/mnt/c/users/mitch/projects/soundproject/soundhunt/sounds'\n",
    "saveToPath='/mnt/c/users/mitch/desktop'\n",
    "path2 = '/mnt/c/users/mitch/projects/soundproject/soundhunt/selectivesounds'\n",
    "\n",
    "titles = []\n",
    "sounds = []\n",
    "for r,d,f in os.walk(path):\n",
    "    for file in f:\n",
    "        if \".wav\" in file:\n",
    "            titles.append(file)\n",
    "            s = wavio.read(path +\"/\"+file)\n",
    "            sounds.append(s)\n",
    "\n",
    "\n",
    "title = {'Title':titles}\n",
    "titlesDF = pb.DataFrame(title)\n",
    "\n",
    "print(titlesDF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to begin the sound files need to be read into the program\n",
    "here we are using wavio which allows the soundfiles to be read in with all their corresponding data points (samples) and their sampling rate\n",
    "\n",
    "we then store this into a dataframe, above is the dataframe for all titles of the incoming sounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sounds[5].data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is an example of what one of our .wav files looks like in its waveform the orange and blue waves indicate that the sound is coming in on both the left and right channels and this is a measure of each\n",
    "\n",
    "It is a good comparison to see how the wave changes overtime as we start extracting features\n",
    "\n",
    "sounds[] is being plotted as sounds.data because each element contains the datapoints of the wave form and the sampling rate, for these graphs the sampling rate will not be useful so we ignore it for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "times = []\n",
    "amplitude = []\n",
    "while i <len(sounds):\n",
    "    curSound = sounds[i]\n",
    "    data = curSound.data\n",
    "    samples = curSound.data.shape[0]\n",
    "    rate = curSound.rate\n",
    "    fftdata = fft(data)\n",
    "    fftdata = abs(fftdata) \n",
    "    curtime = samples/rate\n",
    "    maxampfft = np.amax(fftdata)\n",
    "    times.append(curtime)\n",
    "    amplitude.append(maxampfft)\n",
    "    i = i+1\n",
    "    \n",
    "SoundDF = {'Titles':titles,'Time':times,'Max Amp':amplitude}\n",
    "SoundDF = pb.DataFrame(SoundDF)\n",
    "print(SoundDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above it is the beginning of the feature extraction\n",
    "\n",
    "For right now we want to experiment with feature extraction and are interested in finding the Max amplitude of each sound as well as the duration or time of each sample. This will hopefully let us see if there is a similarity measurement that can be found using only time and max amplitude.\n",
    "\n",
    "To find the time length of each sound is pretty easy, wavio gives us the the songs sampling rate and the number of samples stored in the sound file. To get the time it is just a quick calculation of samples/sampling rate\n",
    "\n",
    "Max Amplitude was a bit more work, in order to get the proper feature extraction a Fast Fourier Transform or FFT was done onto the sound. An FFT takes a sound wave and breaks it into its corresponding frequency domain, this essentially allows better access for applying filters to start removing data that is not needed for us\n",
    "\n",
    "Once the FFT is completed on the data we store it into a numpy array, using a numpy array we can use the .amax() call to find the largest data point in the array, this is what gives us the max amplitude\n",
    "\n",
    "Again is a dataframe from pandas with storing the same sound files but this time with their length of file and the maximum amplitude\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(abs(fft(sounds[5].data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison here is the same soundfile we looked at but with the FFT applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soundData =SoundDF[['Time','Max Amp']]\n",
    "time = soundData[['Time']]\n",
    "amp = soundData[['Max Amp']]\n",
    "plt.plot\n",
    "plt.scatter(time,amp, label='True Position')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the first comparison of the soundfiles put onto a scatterplot above\n",
    "\n",
    "The X axis is our time measurement for each song, the Y is the maximum amplitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=soundData.rename_axis('ID').values\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "kmeans.fit(X)\n",
    "Y = kmeans.predict(X)\n",
    "plt.scatter(X[:,0],X[:,1], c=kmeans.labels_, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was the first attempt at trying to feature extract, compare and cluster the sound data and as seen above there this was not successful at getting similar sounds grouped together\n",
    "\n",
    "There are many reasons why I believe this dataset did not yield a useable result\n",
    "\n",
    "1. To start using just Time and Maximum Amplitude is not enough data to compare songs to each other, two sounds that are explosions may have drastically different values of both Time and Max Amplitude which would categorize them seperatly rather than together\n",
    "\n",
    "2. The Amplitude should have been normalized before trying to compare the files, without normalizing the data will stay the same as it is above and will not give a good result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Approach\n",
    "\n",
    "From the results above it seemed that a new approach was needed there was a few options that we began to try\n",
    "\n",
    "1. Changing the library for how sound files are opened/analyzed\n",
    "At this point we switched to pyaudioanalyze as the main library for both opening and analyzing sound files, wavio is fine for opening and would have also worked it just made sense to switch the opening library to the same one that will also analyze\n",
    "\n",
    "2. Different features to start extracting\n",
    "Researching what are new possible features that could be extracted from sound files there was several options, zero crossing rate, energy, or chromaspectrum, I decided to start with the zero crossing rate\n",
    "\n",
    "The zero crossing rate is a dataset that will determine everytime a wave crosses over 0. I was able to measure this for all sound files but was unsure how to use this data to start comparing so I quickly switched over to chromavector\n",
    "\n",
    "The chromavector is the relation between music and color representation. There are 12 chromavector value measurements \n",
    "\"G#\", \"G\", \"F#\", \"F\", \"E\", \"D#\", \"D\", \"C#\", \"C\", \"B\", \"A#\", \"A\"\n",
    "\n",
    "Basically if a note is an octave apart it will be observed to have a similar colour and when determining a pitch it is broken into two components, pitch height and chromavector. \n",
    "\n",
    "If you break down a sound into these chromavectors to determine the frequency and prevelence of a certain note you can begin comparing sounds together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []\n",
    "titles = []\n",
    "for (path, dirnames, filename) in os.walk(path):\n",
    "    filenames.append(filename)\n",
    "\n",
    "filenames = filenames[0]\n",
    "\n",
    "for sound in filenames:\n",
    "    if '.wav' in sound:\n",
    "        [sr, data] = audioBasicIO.read_audio_file(path + '/' + sound)\n",
    "        feature, feature_categories = ShortTermFeatures.feature_extraction(data[ :, 0 ],sr, 0.050*sr,0.025*sr)\n",
    "        file = sound.replace('.wav','')\n",
    "        titles.append(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So above the same audio files are being read again\n",
    "\n",
    "sr is the sampling rate of the audio file being read\n",
    "data is again the data points of the incoming soundwave\n",
    "\n",
    "there is a new line below it that is giving us 'feature'\n",
    "ShortTermFeatures is a call from the pyaudioanalyze library that takes the data of a sound,its sampling rate and then uses window analysis\n",
    "\n",
    "window analysis uses two variables a window and a step\n",
    "the window is the .050sr\n",
    "the step is the .025sr\n",
    "This method creates windows of a 50ms window to start analyzing the sound data\n",
    "to make sure that no data is lost we specify the step count, in this case half the length of the window, that means every 25ms we create a 50ms wide window that is allowed to look at the data.\n",
    "\n",
    "This 50% overlap makes sure no data is missed and gives a more accurate measurement overall, the smaller the window/step the more data you will take in\n",
    "\n",
    "ShorTermFeatures uses the data and window analysis to first perform a time specific FFT (rather than one taken over the entire sound wave like before) it will then compute several audio features from each sound including the chronovectors that we will need for the comparison\n",
    "\n",
    "it then appends the titles to save for later storage, this time I removed the .wav from the end of each just for cleaner looking dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pb.DataFrame(feature,index = feature_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureDF=pb.DataFrame(feature,index = feature_categories)\n",
    "featureDF.to_csv(saveToPath +\"/featuresOneSound.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we write out the entire feature extraction of one sound it is very large amounts of data points if you would like to see it in full view it in the csv file that it will write to\n",
    "\n",
    "each row is one spectral feature with the given label for what it represents\n",
    "\n",
    "We are only interested in rows 21-32 (chroma_1 - chroma_12) as that is the chromavectors that was mentioned before\n",
    "\n",
    "The additional data is not used in this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sound in filenames:\n",
    "    if '.wav' in sound:\n",
    "        [sr, data] = audioBasicIO.read_audio_file(path + '/' + sound)\n",
    "        feature,_ = ShortTermFeatures.feature_extraction(data[ :, 0 ],sr, 0.050*sr,0.025*sr)\n",
    "        chrono = feature[21]\n",
    "        for i in range(22,33):\n",
    "            temp = feature[i]\n",
    "            chrono = np.vstack([chrono,temp])\n",
    "pb.DataFrame(chrono)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the same as before loading in soundfiles, doing the feature extraction except now we are taking the only part we need which is the chronovector values and then stacking them into the array as chrono this is the same above as the (chroma_1 to chroma_12)\n",
    "\n",
    "This loop ended up taking very long to run when using the data so the following cells get rid of the loop and instead splice each array for the needed cells [21:33] makes runtime much faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ChromaData =pb.DataFrame()\n",
    "for sound in filenames:\n",
    "    if '.wav' in sound:\n",
    "        [sr, data] = audioBasicIO.read_audio_file(path + '/' + sound)\n",
    "        feature, _ = ShortTermFeatures.feature_extraction(data[ :, 0 ],sr, 0.050*sr,0.025*sr)\n",
    "        \n",
    "        chrono = feature[21:33]\n",
    "        \n",
    "        notes = chrono.shape[1]\n",
    "        noteVal = chrono.argmax(axis = 0)\n",
    "        x, bin = np.histogram(noteVal, bins = 12)\n",
    "        normalData = x.reshape( 1, 12 ).astype( float ) / notes\n",
    "        pb.DataFrame(normalData)\n",
    "        nb =pb.DataFrame(normalData)\n",
    "        ChromaData=ChromaData.append(nb,ignore_index =True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here again we extract the Chromovectors needed to calculate the notes\n",
    "However as seen in the DataFrame above there are 686 columns of data for each sound and we need to have 12, 1 for each note representation\n",
    "to do this we start normalizing the data for each song\n",
    "notes = to the amount of data points for the sound in this case again 686 noteval is a value that will give the most prominent note within the sound of the index\n",
    "the histogram keeps track of each note in the sound using the numpy histogram function to track the sounds as the are read\n",
    "the data is then finally normalized by using the histogram data and reshaping it into the 12 note categories\n",
    "finally we store the data into a dataframe =nb for temp storage then append this temp dataframe into the main storage of ChromaData\n",
    "You can see the results below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pb.DataFrame(ChromaData.values,columns = [\"G#\", \"G\", \"F#\", \"F\", \"E\", \"D#\", \"D\", \"C#\", \"C\", \"B\", \"A#\", \"A\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=ChromaData\n",
    "\n",
    "X = df.values\n",
    "\n",
    "model = KMeans(n_clusters = 3)\n",
    "model.fit(X)\n",
    "\n",
    "Y = model.predict(X) # --> 0-max_amount_cluster\n",
    "all_values = np.append(model.cluster_centers_, X, axis=0)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(all_values)\n",
    "X_transformed = pca.transform(X)\n",
    "cluster_centers_transformed = pca.transform(model.cluster_centers_)\n",
    "\n",
    "plt.scatter(X_transformed[:,0], X_transformed[:,1],c=model.labels_,cmap = 'rainbow')\n",
    "plt.scatter(cluster_centers_transformed[:,0], cluster_centers_transformed[:,1],color = 'black')\n",
    "\n",
    "finalDF = {'Title':titles,'Cluster #':model.labels_}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where the Data is placed and clustered into its organized categories\n",
    "\n",
    "Using sklearn kmeans clustering method\n",
    "\n",
    "Kmeans clustering is a form where \"Center points\" are picked at random based on the amount (K) of clusters you pick\n",
    "\n",
    "All data points are measured to these center points and whichever is the closest it will join that cluster\n",
    "\n",
    "After all data points are assigned, each cluster gets a mean solve to determine a new mid point for each cluster, from there the data will be measured and reassigned again\n",
    "\n",
    "This step repeats until there are no changes within the clustering data\n",
    "\n",
    "After feeding the clustering algorithm the data and set amount of clusters in this case 10 each data point will get set to a cluster\n",
    "\n",
    "However in order to visualize the clusters on a 2d graph there runs into an issue\n",
    "\n",
    "Our data so far is a 1x12 dimensional matrix containing all the note values, you can't display this on a cluster graph like above\n",
    "\n",
    "This is where the pca from Sklearn come in (Principle Component Analysis) is a linear dimension reduction, no data is scaled but it will be recentered\n",
    "\n",
    "Also had to make sure that the center points also would be reduced just like the other data points so that the clustering would stay consistent and not linking reduced data to non reduced data\n",
    "\n",
    "The plot above each color is one cluster to show how they spread apart and group together, Black dots are the center spots for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pb.DataFrame(finalDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Data displayed in the cluster graph is not exactly what I was looking for, The data seems to be very spread out but also jumbling several categories together\n",
    "\n",
    "I believe this could be caused due to either the group of sounds I am using being very similar to each other resulting in the large middle cluster meanwhile sounds to the top and right are very different\n",
    "\n",
    "May also be caused by a possible wrong K value, there was yet time to make a proper elbow plot that could give a better idea about how many clusters should be placed in graphic\n",
    "\n",
    "The last reason which I am unsure about is the scale of the graph may be too small for this many sound files, it could be giving off the wrong display however it may be a reach, possibly in the future be able to play around with that factor to decide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []\n",
    "titles = []\n",
    "for (path, dirnames, filename) in os.walk(path2):\n",
    "    filenames.append(filename)\n",
    "\n",
    "filenames = filenames[0]\n",
    "ChromaData =pb.DataFrame()\n",
    "for sound in filenames:\n",
    "    if '.wav' in sound:\n",
    "        [sr, data] = audioBasicIO.read_audio_file(path2 + '/' + sound)\n",
    "        feature, _ = ShortTermFeatures.feature_extraction(data[ :, 0 ],sr, 0.050*sr,0.025*sr)\n",
    "        \n",
    "        file = sound.replace('.wav','')\n",
    "        titles.append(file)\n",
    "        chrono = feature[21:33]\n",
    "        \n",
    "        notes = chrono.shape[1]\n",
    "        noteVal = chrono.argmax(axis = 0)\n",
    "        x, bin = np.histogram(noteVal, bins = 12)\n",
    "        normalData = x.reshape( 1, 12 ).astype( float ) / notes\n",
    "        pb.DataFrame(normalData)\n",
    "        nb =pb.DataFrame(normalData)\n",
    "        ChromaData=ChromaData.append(nb,ignore_index =True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=ChromaData\n",
    "\n",
    "X = df.values\n",
    "model = KMeans(n_clusters = 3)\n",
    "model.fit(X)\n",
    "Y = model.predict(X) # --> 0-max_amount_cluster\n",
    "all_values = np.append(model.cluster_centers_, X, axis=0)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(all_values)\n",
    "X_transformed = pca.transform(X)\n",
    "cluster_centers_transformed = pca.transform(model.cluster_centers_)\n",
    "\n",
    "plt.scatter(X_transformed[:,0], X_transformed[:,1],c=model.labels_,cmap = 'rainbow')\n",
    "plt.scatter(cluster_centers_transformed[:,0], cluster_centers_transformed[:,1],color = 'black')\n",
    "finalDF = {'Title':titles,'Cluster #':model.labels_}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have the same method but on a different smaller dataset, this dataset was a selection of sound I picked with very clear categories from each other, there are penguin sounds, punching sounds and explosions. When this data is clustered here it will make three very clear clusters based on the K I give.\n",
    "\n",
    "So far what this tells me is that using the chromavector for determining sound comparison is still very uncertain. With smaller data sets it appears to work better but also when you have distinctly different sounds the clusters work out very well\n",
    "\n",
    "In comparison the random mix of samples picked in the first data set there are many similar sounds and lots more data to jumble up resulting in less accurate clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pb.DataFrame(finalDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here above shows the Data again just without the plot, The sounds that are similar do get placed together but with a dataset so small and so distinct it is hard to tell if it was really a success or if it was a fault of the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With what I can see from the data it seems that chromavectors are not the only data needed to determine if a sound is similar or different. Within large datasets with a wide range of sounds it appears that sounds which should be paired together seem to split into new clusters away from on an other. It is unclear if maybe a wider graph would help. But additional Data is needed to clarify these questions and hypothesis about what methods can be used for similarity comparison. Maybe combining additonal data like the energy reading or the zero crossing rate can be used to help seperate similar catergorical sounds from each other. In future I would also try to implement an actual comparison reading for each individual soundfile that way they can be related to each other individually rather than just to where they end up in the cluster, but unsure of how to proceed with that idea. It is possible to create some organization of sounds with these methods but it is not accurate enough for a final product"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
